{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM8O764Dr9/WWn/Wn7YFpo9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-pike3/Projects_In_AI-ML/blob/main/Hw3_part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4pd6GiP4Wuf",
        "outputId": "4b860bbf-1a45-4e09-c21c-0d51ccbc94ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers import normalizers, pre_tokenizers, Tokenizer, models, trainers\n",
        "import time\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation\n"
      ],
      "metadata": {
        "id": "df9e2cv7Wc13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TranslationDataset stores instance of English-French translation set and associated tokenizers."
      ],
      "metadata": {
        "id": "BL1BVO6GYVIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, eng_tokenizer=None,fr_tokenizer=None):\n",
        "        super(TranslationDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.values = []\n",
        "        self.labels = []\n",
        "        self.set_values_labels()\n",
        "        self.eng_tokenizer = None\n",
        "        if eng_tokenizer is None:\n",
        "          self.eng_tokenizer = Tokenizer(models.BPE())\n",
        "          self.eng_tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
        "          trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[BOS]\", \"[EOS]\"]) #  Adding [BOS] and [EOS] here\n",
        "          self.eng_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "          self.eng_tokenizer.post_processor = TemplateProcessing(\n",
        "              single=\"[BOS] $A [EOS]\",\n",
        "              special_tokens=[(\"[BOS]\", 1), (\"[EOS]\", 2)],\n",
        "          )\n",
        "          self.eng_tokenizer.train_from_iterator(self.values_iterator(), length=len(self.values))\n",
        "        else:\n",
        "          self.eng_tokenizer = eng_tokenizer\n",
        "\n",
        "        self_fr_tokenizer = None\n",
        "        if fr_tokenizer is None:\n",
        "          self.fr_tokenizer = Tokenizer(models.BPE())\n",
        "          trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[BOS]\", \"[EOS]\"]) #  Adding [BOS] and [EOS] here\n",
        "          self.fr_tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
        "          self.fr_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "          self.fr_tokenizer.post_processor = TemplateProcessing(\n",
        "              single=\"[BOS] $A [EOS]\",\n",
        "              special_tokens=[(\"[BOS]\", 1), (\"[EOS]\", 2)],\n",
        "          )\n",
        "          self.fr_tokenizer.train_from_iterator(self.labels_iterator(), length=len(self.labels))\n",
        "        else:\n",
        "          self.fr_tokenizer = fr_tokenizer\n",
        "        self.fr_max_len = 0\n",
        "        self.set_max_len()\n",
        "\n",
        "\n",
        "    def set_max_len(self):\n",
        "      for i in range(len(self.values)):\n",
        "        seq_len = len(self.fr_tokenizer.encode(self.labels[i]).ids)\n",
        "        if(seq_len > self.fr_max_len):\n",
        "          self.fr_max_len = seq_len\n",
        "    def get_fr_max_len(self):\n",
        "      return self.fr_max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.values)  # number of samples in the dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.eng_tokenizer.encode(self.values[index]).ids), \\\n",
        "            torch.tensor(self.fr_tokenizer.encode(self.labels[index]).ids)\n",
        "\n",
        "    def set_values_labels(self):\n",
        "       for i in range(len(self.data)):\n",
        "        self.values.append(self.data[i]['translation']['en'])\n",
        "        self.labels.append(self.data[i]['translation']['fr'])\n",
        "\n",
        "    def get_values(self):\n",
        "      return self.values\n",
        "\n",
        "    def get_labels(self):\n",
        "      return self.labels\n",
        "\n",
        "    def values_iterator(self):\n",
        "      for i in range(len(self.values)):\n",
        "          yield self.values[i]\n",
        "\n",
        "    def labels_iterator(self):\n",
        "      for i in range(len(self.labels)):\n",
        "          yield self.labels[i]\n",
        "\n",
        "    def get_eng_tokenizer(self):\n",
        "      return self.eng_tokenizer\n",
        "\n",
        "    def get_fr_tokenizer(self):\n",
        "      return self.fr_tokenizer"
      ],
      "metadata": {
        "id": "a5fmEKv83TEr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "0PqC9avGYnNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books = load_dataset(\"opus_books\", \"en-fr\")\n",
        "tmp = books[\"train\"].train_test_split(test_size=0.1)\n",
        "books1 = tmp[\"test\"]\n",
        "books1 = books1.train_test_split(test_size=0.2)\n",
        "train = books1[\"train\"]\n",
        "tmp = books1[\"test\"].train_test_split(test_size=0.5)\n",
        "val = tmp[\"train\"]\n",
        "test = tmp[\"test\"]\n",
        "len(train),len(val),len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9hwZkgu6noa",
        "outputId": "040834a5-76cb-4613-adb4-49e180f016d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10167, 1271, 1271)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = TranslationDataset(train)"
      ],
      "metadata": {
        "id": "4Tjc9sZc7BGg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation and test set uses tokenizers trained from the training set\n",
        "val_iter = TranslationDataset(val, eng_tokenizer=train_iter.get_eng_tokenizer(),fr_tokenizer=train_iter.get_fr_tokenizer())\n",
        "test_iter = TranslationDataset(test,eng_tokenizer=train_iter.get_eng_tokenizer(),fr_tokenizer=train_iter.get_fr_tokenizer())"
      ],
      "metadata": {
        "id": "rzVDQazCRFmo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer\n",
        "\n",
        "Adapted from the following tutorials:\n",
        "\n",
        "Etienne, B. (2024, March 19). A complete guide to write your own transformers. Medium. https://medium.com/data-science/a-complete-guide-to-write-your-own-transformers-29e23f371ddd\n",
        "\n",
        "Sayed, E. (2024, June 11). Building a transformer from scratch: A step-by-step guide. Medium. https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a\n",
        "\n"
      ],
      "metadata": {
        "id": "6tJIyEuFP0kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_heads=2):\n",
        "        \"\"\"\n",
        "        input_dim: Dimensionality of the input.\n",
        "        num_heads: The number of attention heads to split the input into.\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Value part\n",
        "        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Key part\n",
        "        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Query part\n",
        "        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False) # the output layer\n",
        "\n",
        "\n",
        "    def scaled_dot_product_attention(self,query,key,value,attention_mask=None,key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n",
        "        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n",
        "        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n",
        "        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n",
        "        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n",
        "\n",
        "        \"\"\"\n",
        "        d_k = query.size(-1)\n",
        "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
        "        attention_scores = torch.div(torch.matmul(query,key.transpose(-2,-1)),np.sqrt(d_k))\n",
        "\n",
        "        # Attention mask here\n",
        "        if attention_mask is not None:\n",
        "          attention_mask = attention_mask.unsqueeze(0)\n",
        "          attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Key mask here\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, num heads\n",
        "            attention_scores = attention_scores + key_padding_mask\n",
        "\n",
        "\n",
        "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
        "        output = torch.matmul(attention_scores, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n",
        "\n",
        "        return output, attention_scores\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            q,\n",
        "            k,\n",
        "            v,\n",
        "            attention_mask=None,\n",
        "            key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        q : tensor of shape (batch_size, query_sequence_length, hidden_dim)\n",
        "        k : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n",
        "        v : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n",
        "        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n",
        "        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n",
        "\n",
        "        \"\"\"\n",
        "        dk = q.size(-1)\n",
        "        q = self.Wq(q)\n",
        "        k = self.Wk(k)\n",
        "        v = self.Wv(v)\n",
        "\n",
        "        q =  q.view(q.shape[0], q.shape[1], self.num_heads, dk // self.num_heads).transpose(1, 2)\n",
        "        k = k.view(k.shape[0], k.shape[1], self.num_heads, dk // self.num_heads).transpose(1, 2)\n",
        "        v = v.view(v.shape[0], v.shape[1], self.num_heads, dk // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        attn_values, attn_weights  = self.scaled_dot_product_attention(\n",
        "            query=q,\n",
        "            key=k,\n",
        "            value=v,\n",
        "            attention_mask=attention_mask,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "        )\n",
        "\n",
        "        batch_size, num_heads, seq_length, head_hidden_dim = attn_values.size()\n",
        "        combined_attn = attn_values.transpose(1, 2).contiguous().view(batch_size, seq_length, self.num_heads * head_hidden_dim)\n",
        "        output = self.Wo(combined_attn)\n",
        "\n",
        "        self.attention_weigths = attn_weights\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "j0reqaBYvM8N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "TjU24xOSvXeZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "7qpHJz58va0i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, n_dim: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = nn.LayerNorm(n_dim)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "RnK3xKVLV29x"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n",
        "        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(n_dim, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        x = self.residual_connections[0](x, lambda x: self.mha(x, x, x,  key_padding_mask=src_mask))\n",
        "        x = self.residual_connections[1](x, self.ff)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xRkZH5GeVD06"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "            vocab_size: int,\n",
        "            n_dim: int,\n",
        "            dropout: float,\n",
        "            n_encoder_blocks: int,\n",
        "            n_heads: int):\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_dim = n_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=n_dim\n",
        "        )\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=n_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderBlock(n_dim, dropout, n_heads) for _ in range(n_encoder_blocks)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(n_dim)\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        x = self.embedding(x) * math.sqrt(self.n_dim)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, padding_mask)\n",
        "        return x\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "FmXTVePCVF4u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n",
        "        self.cross_mha = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n",
        "        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(n_dim, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):\n",
        "        x = self.residual_connections[0](x, lambda x: self.mha(x, x, x, attention_mask=tgt_mask, key_padding_mask=tgt_padding_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_mha(x, encoder_output, encoder_output, attention_mask=None, key_padding_mask=memory_padding_mask))\n",
        "        x = self.residual_connections[2](x, self.ff)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, n_dim: int, dropout: float, n_decoder_blocks: int, n_heads: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=n_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=n_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(n_dim, dropout, n_heads) for _ in range(n_decoder_blocks)\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):\n",
        "        x = self.embedding(tgt)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(\n",
        "                x,\n",
        "                memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                tgt_padding_mask=tgt_padding_mask,\n",
        "                memory_padding_mask=memory_padding_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5_yAGEyaOTXR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(size: int):\n",
        "      \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n",
        "      mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n",
        "      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "      return mask"
      ],
      "metadata": {
        "id": "4TPsnCKfvtvM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, model_dim: int, dropout: float, n_encoder_layers: int, n_decoder_layers: int, n_heads: int):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.model_dim = model_dim\n",
        "        self.dropout = dropout\n",
        "        self.n_encoder_layers = n_encoder_layers\n",
        "        self.n_decoder_layers = n_decoder_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.PAD_IDX = 0\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            self.vocab_size, self.model_dim, self.dropout, self.n_encoder_layers, self.n_heads)\n",
        "        self.decoder = Decoder(\n",
        "            self.vocab_size, self.model_dim, self.dropout, self.n_decoder_layers, self.n_heads)\n",
        "        self.fc = nn.Linear(self.model_dim, self.vocab_size)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_square_subsequent_mask(size: int):\n",
        "            \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n",
        "            mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n",
        "            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "            return mask\n",
        "\n",
        "\n",
        "    def encode(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "        ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Input\n",
        "            x: (B, S) with elements in (0, C) where C is num_classes\n",
        "        Output\n",
        "            (B, S, E) embedding\n",
        "        \"\"\"\n",
        "        mask = (x == self.PAD_IDX).float()\n",
        "        encoder_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "        # (B, S, E)\n",
        "        encoder_output = self.encoder(\n",
        "            x,\n",
        "            padding_mask=encoder_padding_mask\n",
        "        )\n",
        "\n",
        "        return encoder_output, encoder_padding_mask\n",
        "\n",
        "\n",
        "    def decode(\n",
        "            self,\n",
        "            tgt: torch.Tensor,\n",
        "            memory: torch.Tensor,\n",
        "            memory_padding_mask=None\n",
        "        ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        B = Batch size\n",
        "        S = Source sequence length\n",
        "        L = Target sequence length\n",
        "        E = Model dimension\n",
        "\n",
        "        Input\n",
        "            encoded_x: (B, S, E)\n",
        "            y: (B, L) with elements in (0, C) where C is num_classes\n",
        "        Output\n",
        "            (B, L, C) logits\n",
        "        \"\"\"\n",
        "\n",
        "        mask = (tgt == self.PAD_IDX).float()\n",
        "        tgt_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "\n",
        "        decoder_output = self.decoder(\n",
        "            tgt=tgt,\n",
        "            memory=memory,\n",
        "            tgt_mask=self.generate_square_subsequent_mask(tgt.size(1)),\n",
        "            tgt_padding_mask=tgt_padding_mask,\n",
        "            memory_padding_mask=memory_padding_mask,\n",
        "        )\n",
        "        output = self.fc(decoder_output)  # shape (B, L, C)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "            y: torch.Tensor,\n",
        "        ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Input\n",
        "            x: (B, Sx) with elements in (0, C) where C is num_classes\n",
        "            y: (B, Sy) with elements in (0, C) where C is num_classes\n",
        "        Output\n",
        "            (B, L, C) logits\n",
        "        \"\"\"\n",
        "\n",
        "        # Encoder output shape (B, S, E)\n",
        "        encoder_output, encoder_padding_mask = self.encode(x)\n",
        "\n",
        "        # Decoder output shape (B, L, C)\n",
        "        decoder_output = self.decode(\n",
        "            tgt=y,\n",
        "            memory=encoder_output,\n",
        "            memory_padding_mask=encoder_padding_mask\n",
        "        )\n",
        "\n",
        "        return decoder_output"
      ],
      "metadata": {
        "id": "fTd-pqspvxlz"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Transformer"
      ],
      "metadata": {
        "id": "xKjShKRTaxK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/data-science/a-complete-guide-to-write-your-own-transformers-29e23f371ddd\n",
        "\n",
        "from tqdm import tqdm\n",
        "PAD_IDX = 0\n",
        "SOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "\n",
        "def train(model, optimizer, loader, loss_fn, epoch):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    history_loss = []\n",
        "    history_acc = []\n",
        "    print('1')\n",
        "\n",
        "    with tqdm(loader, position=0, leave=True) as tepoch:\n",
        "        for x, y in tepoch:\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(x, y[:, :-1])\n",
        "            loss = loss_fn(logits.contiguous().view(-1, model.vocab_size), y[:, 1:].contiguous().view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            losses += loss.item()\n",
        "\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            masked_pred = preds * (y[:, 1:]!=PAD_IDX)\n",
        "            accuracy = (masked_pred == y[:, 1:]).float().mean()\n",
        "            acc += accuracy.item()\n",
        "\n",
        "            history_loss.append(loss.item())\n",
        "            history_acc.append(accuracy.item())\n",
        "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy.item())\n",
        "\n",
        "    return losses / len(list(loader)), acc / len(list(loader)), history_loss, history_acc\n",
        "\n",
        "\n",
        "def evaluate(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    acc = 0\n",
        "    history_loss = []\n",
        "    history_acc = []\n",
        "\n",
        "    for x, y in tqdm(loader, position=0, leave=True):\n",
        "\n",
        "        logits = model(x, y[:, :-1])\n",
        "\n",
        "        loss = loss_fn(logits.contiguous().view(-1, model.vocab_size), y[:, 1:].contiguous().view(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        masked_pred = preds * (y[:, 1:]!=PAD_IDX)\n",
        "        accuracy = (masked_pred == y[:, 1:]).float().mean()\n",
        "        acc += accuracy.item()\n",
        "\n",
        "        history_loss.append(loss.item())\n",
        "        history_acc.append(accuracy.item())\n",
        "\n",
        "    return losses / len(list(loader)), acc / len(list(loader)), history_loss, history_acc"
      ],
      "metadata": {
        "id": "RQTIHMlnv7FI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    This function pads inputs with PAD_IDX to have batches of equal length\n",
        "    \"\"\"\n",
        "    src_batch, tgt_batch = [], []\n",
        "\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(src_sample)\n",
        "        tgt_batch.append(tgt_sample)\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "UBKI-5Xn1Y9K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/data-science/a-complete-guide-to-write-your-own-transformers-29e23f371ddd\n",
        "# Model definition\n",
        "model = Transformer(vocab_size=train_iter.get_fr_tokenizer().get_vocab_size(),\n",
        "    model_dim=128,\n",
        "    dropout=0.1,\n",
        "    n_encoder_layers=2,\n",
        "    n_decoder_layers=2,\n",
        "    n_heads=2)\n",
        "\n",
        "# Instantiate datasets\n",
        "dataloader_train = DataLoader(train_iter, batch_size=8, collate_fn=collate_fn)\n",
        "dataloader_val = DataLoader(val_iter, batch_size=8, collate_fn=collate_fn)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "# Define loss function : we ignore logits which are padding tokens\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Save history to dictionnary\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'eval_loss': [],\n",
        "    'train_acc': [],\n",
        "    'eval_acc': []\n",
        "}\n",
        "\n",
        "# Main loop\n",
        "for epoch in range(1, 5):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc, hist_loss, hist_acc = train(model, optimizer, dataloader_train, loss_fn, epoch)\n",
        "\n",
        "    history['train_loss'] += hist_loss\n",
        "    history['train_acc'] += hist_acc\n",
        "    end_time = time.time()\n",
        "    val_loss, val_acc, hist_loss, hist_acc = evaluate(model, dataloader_val, loss_fn)\n",
        "    history['eval_loss'] += hist_loss\n",
        "    history['eval_acc'] += hist_acc\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Train acc: {train_acc:.3f}, Val loss: {val_loss:.3f}, Val acc: {val_acc:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g3HPmjXFOxq",
        "outputId": "e554ebf6-441e-40c4-e700-17f043aed9ae"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 1271/1271 [15:24<00:00,  1.37it/s, accuracy=73.2, loss=5.76]\n",
            "100%|██████████| 159/159 [00:56<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 6.099, Train acc: 0.600, Val loss: 5.806, Val acc: 0.627 Epoch time = 931.291s\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 1271/1271 [15:39<00:00,  1.35it/s, accuracy=75, loss=5.32]\n",
            "100%|██████████| 159/159 [00:46<00:00,  3.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Train loss: 5.186, Train acc: 0.632, Val loss: 5.512, Val acc: 0.644 Epoch time = 945.293s\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 1271/1271 [15:47<00:00,  1.34it/s, accuracy=76, loss=4.98]\n",
            "100%|██████████| 159/159 [00:46<00:00,  3.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Train loss: 4.733, Train acc: 0.653, Val loss: 5.513, Val acc: 0.650 Epoch time = 953.533s\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 1271/1271 [15:40<00:00,  1.35it/s, accuracy=76.4, loss=4.73]\n",
            "100%|██████████| 159/159 [00:47<00:00,  3.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Train loss: 4.422, Train acc: 0.672, Val loss: 5.442, Val acc: 0.655 Epoch time = 946.265s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation"
      ],
      "metadata": {
        "id": "7KSC0DsHR8FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(nn.Module):\n",
        "    def __init__(self, transformer, eng_tokenizer, fr_tokenizer):\n",
        "        super(Translator, self).__init__()\n",
        "        self.transformer = transformer\n",
        "        self.eng_tokenizer = eng_tokenizer\n",
        "        self.fr_tokenizer = fr_tokenizer\n",
        "\n",
        "    def __call__(self, sentence, max_length=None, pad=False):\n",
        "\n",
        "        x = torch.tensor(self.eng_tokenizer.encode(sentence).ids).reshape(1,-1)\n",
        "        encoder_output, mask = model.encode(x) # (B, S, E)\n",
        "        if not max_length:\n",
        "            max_length = x.size(1)\n",
        "\n",
        "        outputs = torch.ones((x.size()[0], max_length)).type_as(x).long() * SOS_IDX\n",
        "\n",
        "        for step in range(1, max_length):\n",
        "            y = outputs[:, :step]\n",
        "            probs = self.transformer.decode(y, encoder_output)\n",
        "            output = torch.argmax(probs, dim=-1)\n",
        "\n",
        "            if output[:, -1].detach().numpy() in (EOS_IDX, SOS_IDX):\n",
        "                break\n",
        "            outputs[:, step] = output[:, -1]\n",
        "\n",
        "        translation = \"\"\n",
        "        for i in range(len(outputs[0])):\n",
        "            translation += self.fr_tokenizer.id_to_token(outputs[0][i])\n",
        "            translation += \" \"\n",
        "\n",
        "        return translation\n",
        "\n",
        "translator = Translator(model,train_iter.get_eng_tokenizer(), train_iter.get_fr_tokenizer())"
      ],
      "metadata": {
        "id": "bFf1aa4_1xGl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am tired\"\n",
        "out = translator(sentence)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WYv4ozs62BRy",
        "outputId": "626690e3-17bf-4cb4-b7b2-0af84722b241"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" Je suis . \" '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "true = val_iter.get_values()\n",
        "scores = []\n",
        "for i in range(len(true)):\n",
        "  guess = [translator(true[i])]\n",
        "  scores.append(sentence_bleu([true[i]], guess))\n",
        "  if(i == 500):\n",
        "    break\n",
        "mean_score = np.mean(scores)\n",
        "print(mean_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9j9zCRuZ2Mf",
        "outputId": "ed2ee155-2260-40ba-8b0d-726d27deb4b5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see from above that the model translates \"I am tired\" to \"Je suis.\" (I am.). Thus, although it is not able to translate accurately, it demonstrates some basic understanding. The bleu score indicates that the transformer model has very low performance, although this is likely due to a lack of training time.\n",
        "\n",
        "Although I was not able to compare my transformer with my encoder-decoder, it seems probable that the encoder decoder model would train faster due to decreased model complexity but would be less accurate because it is a less sophisticated model."
      ],
      "metadata": {
        "id": "hHmwxeE7s3Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Decoder"
      ],
      "metadata": {
        "id": "tM1BocF1WU0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(self,query,key,value):\n",
        "        d_k = query.size(-1)\n",
        "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
        "        attention_scores = torch.div(torch.matmul(query,key.transpose(-2,-1)),np.sqrt(d_k))\n",
        "\n",
        "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
        "        output = torch.matmul(attention_scores, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n",
        "\n",
        "        return output, attention_scores"
      ],
      "metadata": {
        "id": "nWqVADkckAZk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size, encoder):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = encoder\n",
        "    self.decoder = nn.LSTM(hidden_size,output_size)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input_tensor, target_tensor):\n",
        "\n",
        "    encoder_output, encoder_hidden = self.encoder(input_tensor)\n",
        "    decoder_output, = self.decoder(encoder_output)\n",
        "    output = self.fc(decoder_output)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "GMixoNLM_WP0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = num_hiddens\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, self.hidden_size,batch_first=True)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        embs = self.embedding(X.t())\n",
        "        context, attn_weights = scaled_dot_product_attention(embs,embs,embs)\n",
        "        h_0 = torch.zeros(1, embs.size(0), self.hidden_size)\n",
        "        outputs, state = self.rnn(embs,h_0)\n",
        "        return outputs, state"
      ],
      "metadata": {
        "id": "4qqQKIhldPTy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Works Cited"
      ],
      "metadata": {
        "id": "70Nw7QuZsozQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etienne, B. (2024, March 19). A complete guide to write your own transformers. Medium. https://medium.com/data-science/a-complete-guide-to-write-your-own-transformers-29e23f371ddd\n",
        "\n",
        "\n",
        "lhoestq. (2021, February 15). NLP dataset for Byteleveltokenizer training. Hugging Face Forums. https://discuss.huggingface.co/t/nlp-dataset-for-byteleveltokenizer-training/3653\n",
        "\n",
        "\n",
        "lianghsun. (2022, August 22). Add Bos and Eos when encoding a sentence. Hugging Face Forums. https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833/2\n",
        "\n",
        "\n",
        "huggingface. (n.d.). Quicktour. Tokenizers. https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#post-processing\n",
        "\n",
        "Sayed, E. (2024, June 11). Building a transformer from scratch: A step-by-step guide. Medium. https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a\n",
        "\n",
        "\n",
        "Zhang, A., Lipton, Z., Li, M., & Smola, A. (2023). Dive into Deep Learning. Cambridge University Press.\n"
      ],
      "metadata": {
        "id": "Rah2NQypsRfz"
      }
    }
  ]
}