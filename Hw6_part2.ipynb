{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPhh/pHfYKNvkiYwucpCtJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-pike3/Projects_In_AI-ML/blob/main/Hw6_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1"
      ],
      "metadata": {
        "id": "XDLw4fKZi4-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One application of reinforcement learning (RL) is in financial trading. Specifically, we can use RL to determine the optimal trading strategy for buying or selling a portfolio of stocks. The paper “Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy” (Yang et al., 2020),  defines a Markov Decision Process (MDP) for this problem as follows:\n",
        "\n",
        "State: A vector that includes stock prices for each stock, the number of shares of each stock held in the portfolio, and the remaining cash balance.\n",
        "Actions: A vector that represents all possible actions (buy, sell, hold) for each stock in the portfolio.\n",
        "Transition Model: At each state, the agent can either buy, sell, or hold each stock in the portfolio. Buying increases the number of shares held by a specified quantity (k), as recommended by the agent. Conversely, selling decreases the number of shares held by k, while holding results in no change in the number of shares.\n",
        "Reward: The reward is calculated as the change in net worth resulting from the actions taken in the previous time step. This is determined by taking the difference  between the current net worth (portfolio value plus cash balance) and the previous net worth plus transaction costs incurred in the current time step."
      ],
      "metadata": {
        "id": "jIvOlC3ki4CK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2"
      ],
      "metadata": {
        "id": "d4qUqq0ji63n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the field of financial trading, reinforcement learning can be applied to develop an optimal trading strategy. The article titled \"Deep Reinforcement Learning for Automated Stock Trading\" is based on the similarly named paper “Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy\" (Yang et al., 2020). This article presents an implementation of an ensemble method to determine the best trading strategy for a portfolio.\n",
        "\n",
        "The project focuses on 30 Dow Jones stocks that were selected for their sufficient liquidity. The data is retrieved from Yahoo Finance (using the yfinance library) and includes features such as open, high, low, close, adjusted close prices, and volume. Indicators such as the Moving Average Convergence Divergence (MACD), Relative Strength Index (RSI), Commodity Channel Index (CCI), and Average Directional Index (ADX) are added to provide deeper insights into market trends and price dynamics. The dataset is split into three time series for training, validation, and testing, with approximately 61% for training, 9% for validation, and 30% for testing.\n",
        "\n",
        "The model features an ensemble agent composed of five individual agents: Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), Soft Actor-Critic (SAC), and Twin Delayed Deep Deterministic Policy Gradient (TD3). According to the original paper, A2C improves the policy update process by reducing the variance of the policy gradient, which enhances the model's robustness. DDPG aims to maximize investment returns, while PPO is chosen to increase the stability of each training update by discouraging drastic policy changes (Yang et al., 2020). Each agent is trained individually for a predetermined number of timesteps. Finally, an ensemble agent is created by averaging the recommendations from the five trained agents.\n",
        "\n",
        "The project utilizes the OpenAI Gym framework to create a trading environment. At each time step, an agent can monitor current stock prices, account balances, shares held, and the overall net worth of the portfolio. With this information, the agent selects a continuous value indicating how much to buy or sell of each stock. The portfolio is updated accordingly, and the reward is calculated as the difference between the  initial balance and the current net worth of the portfolio.\n",
        "\n",
        "For evaluation, all six agents (the five individual agents and the ensemble agent) can be compared using the Sharpe ratio, which measures an agent's returns relative to the risk it assumes. Using this metric, agents rank as follows: SAC, TD3, A2C, DDPG, Ensemble, PPO. Thus, most individual agents outperformed the ensemble strategy. In the future, learning an optimal combination of these agents—rather than simply using the mean prediction—may lead to improvements in the performance of the ensemble agent.\n",
        "\n",
        "The final result can be used to devise a trading strategy for the next day’s portfolio."
      ],
      "metadata": {
        "id": "dyPCzD_Noe59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Works Cited"
      ],
      "metadata": {
        "id": "Mq7J4KtXi9GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gino, S. (2024, August 24). Deep Reinforcement Learning for Automated Stock Trading. Medium. https://sgino209.medium.com/deep-reinforcement-learning-for-automated-stock-trading-c661299ebe0f\n",
        "\n",
        "Yang, Hongyang, et al. “Deep reinforcement learning for automated stock trading: An ensemble strategy.” Proceedings of the first ACM international conference on AI in finance. 2020."
      ],
      "metadata": {
        "id": "025GBvJ2olp9"
      }
    }
  ]
}